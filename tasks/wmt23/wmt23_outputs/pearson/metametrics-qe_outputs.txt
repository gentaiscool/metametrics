lang:                                  en-de,he-en,zh-en     en-de     en-de     en-de     he-en     he-en     he-en     zh-en     zh-en     zh-en
level:                                               sys       sys       seg       seg       sys       seg       seg       sys       seg       seg
corr_fcn:                                       accuracy   pearson   pearson     acc-t   pearson   pearson     acc-t   pearson   pearson     acc-t
metric                       avg-corr              task1     task2     task3     task4     task5     task6     task7     task8     task9    task10
---------------------------  --------  -----------------  --------  --------  --------  --------  --------  --------  --------  --------  --------
XCOMET-Ensemble               1 0.825            2 0.928   5 0.980   1 0.695   2 0.604   3 0.950   1 0.556   1 0.586   8 0.927   2 0.650   1 0.543
XCOMET-QE-Ensemble[noref]     2 0.808            4 0.908   9 0.974   2 0.679   5 0.588  13 0.909   4 0.498   6 0.554  14 0.892   4 0.647   4 0.533
MetricX-23                    3 0.808            3 0.908   7 0.977   4 0.585   3 0.603  12 0.910   2 0.548   3 0.577  17 0.873   5 0.625   5 0.531
GEMBA-MQM[noref]              4 0.802            1 0.944   1 0.993  10 0.502  10 0.572   5 0.939   8 0.401   5 0.564   1 0.991  10 0.449   8 0.522
metametrics-qe                5 0.801            5 0.908  18 0.934   6 0.556   1 0.609  26 0.815   5 0.474   2 0.578  12 0.900   1 0.660   3 0.537
MetricX-23-QE[noref]          6 0.800           13 0.892  12 0.969   3 0.626   4 0.596  23 0.858   3 0.520   4 0.564  19 0.859   3 0.647   6 0.527
mbr-metricx-qe[noref]         7 0.788           18 0.880   8 0.976   5 0.571   6 0.584  10 0.915   7 0.411   7 0.553   6 0.936   8 0.489   2 0.537
MaTESe                        8 0.782            8 0.904  25 0.918   7 0.554  20 0.528  15 0.906   6 0.459   8 0.550  15 0.889   7 0.511  25 0.479
_CometKiwi[noref]             9 0.782            7 0.904  16 0.946  12 0.475  11 0.569  22 0.860  13 0.387   9 0.544   3 0.963  12 0.442   7 0.525
_COMET                       10 0.779           10 0.900   3 0.990  17 0.432   8 0.574   4 0.940   9 0.401  11 0.532  13 0.898  14 0.396  12 0.514
_BLEURT-20                   11 0.776           12 0.892   4 0.990  11 0.484   9 0.572   6 0.937  16 0.382  15 0.519  16 0.880  17 0.378   9 0.518
KG-BERTScore[noref]          12 0.774           16 0.884  19 0.926  13 0.451  14 0.556  14 0.908  15 0.382  10 0.537   4 0.962  13 0.430  10 0.516
sescoreX                     13 0.772           14 0.892  15 0.952   8 0.519  12 0.563  17 0.901  14 0.385  23 0.484  23 0.797   6 0.536  15 0.499
cometoid22-wmt22[noref]      14 0.772           17 0.880  10 0.973  15 0.441   7 0.578  25 0.839  18 0.365  16 0.515   5 0.940   9 0.479  11 0.515
_docWMT22CometDA             15 0.768            9 0.904   2 0.990  20 0.394  13 0.559   8 0.922  19 0.339  21 0.497   9 0.907  19 0.353  19 0.493
_docWMT22CometKiwiDA[noref]  16 0.767           11 0.900  11 0.970  14 0.444  15 0.547  16 0.906  23 0.286  22 0.489   2 0.965  16 0.387  18 0.493
Calibri-COMET22              17 0.767            6 0.904  13 0.963  19 0.413  24 0.522   7 0.930  10 0.401  17 0.515  18 0.863  15 0.396  27 0.474
Calibri-COMET22-QE[noref]    18 0.755           22 0.863   6 0.978  16 0.441  31 0.483  28 0.778  12 0.395  19 0.506   7 0.934  11 0.443  20 0.491
_YiSi-1                      19 0.754           21 0.871  20 0.925  21 0.366  17 0.542   9 0.917  11 0.395  12 0.529  20 0.823  20 0.290  13 0.504
_MS-COMET-QE-22[noref]       20 0.744           20 0.871  14 0.959  23 0.310  16 0.546  31 0.721  22 0.295  20 0.498  11 0.901  18 0.367  17 0.498
_prismRef                    21 0.744           25 0.851  22 0.920   9 0.516  28 0.518   2 0.956  21 0.319  13 0.528  28 0.762  23 0.183  14 0.504
mre-score-labse-regular      22 0.743           15 0.888  17 0.942  31 0.111  18 0.530   1 0.958  17 0.378  14 0.522  10 0.903  25 0.145  23 0.481
_BERTscore                   23 0.742           19 0.871  27 0.891  22 0.325  21 0.528  18 0.895  20 0.335  18 0.515  21 0.810  21 0.236  16 0.499
XLsim                        24 0.719           24 0.855  21 0.925  25 0.239  22 0.527  19 0.887  25 0.233  24 0.480  24 0.796  27 0.111  30 0.464
_f200spBLEU                  25 0.704           28 0.819  23 0.919  26 0.237  23 0.526  27 0.805  26 0.230  27 0.447  27 0.772  28 0.108  26 0.476
MEE4                         26 0.704           27 0.823  29 0.861  29 0.202  19 0.529  20 0.879  24 0.256  31 0.441  29 0.743  29 0.105  24 0.480
tokengram_F                  27 0.703           30 0.815  31 0.858  28 0.227  25 0.520  21 0.878  27 0.226  25 0.461  25 0.795  31 0.060  22 0.485
embed_llama                  28 0.701           26 0.831  30 0.861  24 0.250  30 0.483  24 0.841  30 0.215  32 0.430  26 0.785  24 0.161  31 0.447
_BLEU                        29 0.696           29 0.815  26 0.917  30 0.192  26 0.520  30 0.769  29 0.220  29 0.442  30 0.734  26 0.119  29 0.472
_chrF                        30 0.694           31 0.795  28 0.866  27 0.232  27 0.519  29 0.776  28 0.221  26 0.460  22 0.809  30 0.063  21 0.485
eBLEU                        31 0.692           23 0.859  24 0.918  33-0.011  29 0.512  11 0.911  32 0.131  28 0.445  31 0.727  33-0.084  28 0.473
_Random-sysname[noref]       32 0.529           32 0.578  32 0.357  32 0.064  33 0.409  32 0.209  33 0.041  33 0.428  32 0.093  32 0.018  33 0.381
_prismSrc[noref]             33 0.455           33 0.386  33-0.327  18 0.425  32 0.426  33-0.017  31 0.140  30 0.441  33-0.406  22 0.223  32 0.421
