{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p30R5QSLqO0R"
      },
      "source": [
        "Colab to reproduce results from the WMT23 metrics shared task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O13agE8mqIJ1"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m23yLnHgEtAA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Install MTME\n",
        "\n",
        "!git clone https://github.com/google-research/mt-metrics-eval.git \u0026\u0026 cd mt-metrics-eval \u0026\u0026 pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvjDTSiEp8zn"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TEt4P13H2Iz"
      },
      "outputs": [],
      "source": [
        "# @title Download data\n",
        "\n",
        "data.Download()  # Copies about 2G onto local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXLFpZ9uiph"
      },
      "source": [
        "## Reproduce official results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le9SHpFHqtmG"
      },
      "outputs": [],
      "source": [
        "# @title Generate main results\n",
        "\n",
        "# Generate main results for primary metrics.\n",
        "\n",
        "# Setting k=0 suppresses significance testing. Results in the paper were\n",
        "# generated with k=1000, which is too slow to run sequentially in a colab.\n",
        "main_tasks, main_task_weights = tasks.WMT23(k=0)\n",
        "\n",
        "# Task names show attributes that define each task.\n",
        "for i, task in enumerate(main_tasks):\n",
        "  print(f'task{i + 1}: {task.name}')\n",
        "\n",
        "# Takes about 3 minutes.\n",
        "main_results = main_tasks.Run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBrVhfNlufDI"
      },
      "outputs": [],
      "source": [
        "# @title Display main results\n",
        "\n",
        "# This reproduces Tables 8 and 9 from the shared task paper, modulo signficance\n",
        "# results.\n",
        "\n",
        "# AverageCorrMatrix produces significance clusters and pairwise p-values for the\n",
        "# overall average correlation, but requires that the tasks be run with k \u003e 0.\n",
        "# AverageCorrs computes the same averages as AverageCorrMatrix but without\n",
        "# significance.\n",
        "avg_corrs = main_results.AverageCorrs(main_task_weights)\n",
        "# avg_corrs, matrix = main_results.AverageCorrMatrix(main_task_weights)\n",
        "\n",
        "# Use fmt='tsv' to generate tsv format for spreadsheets. This function has\n",
        "# many other options to customize output.\n",
        "table = main_results.Table(\n",
        "    metrics=list(avg_corrs),\n",
        "    initial_column=avg_corrs,\n",
        "    initial_column_header='avg-corr',\n",
        "    attr_list=['lang', 'level', 'corr_fcn'],\n",
        "    nicknames={'KendallWithTiesOpt': 'acc-t'},\n",
        "    fmt='text',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqR-lsYuI7tT"
      },
      "outputs": [],
      "source": [
        "# @title Generate full results\n",
        "\n",
        "# Identical to main results except we include contrastive  metric submissions.\n",
        "\n",
        "main_tasks_full, _ = tasks.WMT23(k=0, primary=False)\n",
        "\n",
        "# Takes about 5 minutes.\n",
        "main_results_full = main_tasks_full.Run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtLazjtdJwvk"
      },
      "outputs": [],
      "source": [
        "# @title Display full results.\n",
        "\n",
        "# This reproduces results from Tables 16 and 17 in the paper.\n",
        "\n",
        "avg_corrs = main_results_full.AverageCorrs(main_task_weights)\n",
        "\n",
        "# Leading *s indicate contrastive submissions, leading _s indicate baselines.\n",
        "table = main_results_full.Table(\n",
        "    metrics=list(avg_corrs),\n",
        "    initial_column=avg_corrs,\n",
        "    initial_column_header='avg-corr',\n",
        "    attr_list=['lang', 'level', 'corr_fcn'],\n",
        "    nicknames={'KendallWithTiesOpt': 'acc-t'},\n",
        "    fmt='text',\n",
        "    which_metrics='union',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHx-r4Is1xKn"
      },
      "outputs": [],
      "source": [
        "# @title Generate DA results\n",
        "\n",
        "# Results for all metrics using DA-SQM instead of MQM as gold scores.\n",
        "\n",
        "# DA scores are available for a wider set of languages than the ones used for\n",
        "# the main evaluation. Only en-de and zh-en are common to both.\n",
        "da_lps = ['cs-uk', 'de-en', 'en-cs', 'en-de', 'en-ja', 'en-zh', 'ja-en' 'zh-en']\n",
        "da_tasks, da_wts = tasks.WMT23(k=0, primary=False, lps=da_lps, gold='da-sqm')\n",
        "\n",
        "for task in da_tasks:\n",
        "  print(task.name)\n",
        "\n",
        "# Takes about 15 minutes.\n",
        "da_results = da_tasks.Run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4DUuFd33Zfv"
      },
      "outputs": [],
      "source": [
        "# @title Display DA results\n",
        "\n",
        "# This reproduces results from tables 19 to 27 in the paper.\n",
        "\n",
        "avg_corrs = da_results.AverageCorrs(da_wts)\n",
        "all_da_lps = ','.join(sorted(da_lps))\n",
        "\n",
        "table = da_results.Table(\n",
        "    metrics=list(avg_corrs),\n",
        "    initial_column=avg_corrs,\n",
        "    initial_column_header='avg-corr',\n",
        "    attr_list=['lang', 'level', 'corr_fcn'],\n",
        "    nicknames={'KendallWithTiesOpt': 'acc-t', all_da_lps: 'all'},\n",
        "    fmt='text',\n",
        "    which_metrics='union',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRuoJY8f8gfd"
      },
      "outputs": [],
      "source": [
        "# @title Accuracy results, MQM vs DA\n",
        "\n",
        "# This reproduces results from table 14 in the paper. Note that the two columns\n",
        "# are not comparable because they are computed on different sets of languages\n",
        "# (in addition to using different gold scores).\n",
        "\n",
        "acc_mqm = main_results.SplitByAttr('corr_fcn')['accuracy']\n",
        "acc_da = da_results.SplitByAttr('corr_fcn')['accuracy']\n",
        "acc_mqm_vs_da = acc_mqm + acc_da\n",
        "\n",
        "table = acc_mqm_vs_da.Table(\n",
        "    attr_list=['lang'],\n",
        "    nicknames={all_da_lps: 'all-DA-lps'},\n",
        "    rerank=[True, True],\n",
        "    which_metrics='intersection',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAfC_WDg4sy"
      },
      "source": [
        "# Evaluate a new metric\n",
        "\n",
        "This section shows a worked example of evaluating a new metric online. Another\n",
        "possibility is to generate scores offline, write score files to disk, and use\n",
        "EvalSet.AddMetricsFromDir() to read them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHA97g1hjKR9"
      },
      "outputs": [],
      "source": [
        "# @title Define the metric\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Replace this function with your own metric.\n",
        "\n",
        "def NewMetric(\n",
        "    level: str,\n",
        "    lp: str,\n",
        "    domains: dict[str, list[list[int]]],\n",
        "    docs: dict[str, list[int]],\n",
        "    src: list[str],\n",
        "    ref: list[str],\n",
        "    hyps: dict[list[str]]\n",
        ") -\u003e dict[str, list[float]]:\n",
        "  \"\"\"\n",
        "  Generate metric scores.\n",
        "\n",
        "  Args:\n",
        "    level: Level for which to produce scores, 'sys' or 'seg'.\n",
        "    lp: Language pair, eg 'en-de'.\n",
        "    domains: Map from domain name to [[beg, end+1], ...] segment position lists.\n",
        "    docs: Map from doc name to [beg, end+1] segment positions.\n",
        "    src: List of source segments.\n",
        "    ref: List of reference segments.\n",
        "    hyps: Map from MT system name to output segments for that system.\n",
        "\n",
        "  Returns:\n",
        "    Map from system name to scores, a list of segment-level scores if level is\n",
        "    'seg', or a list containing a single score if level is 'sys'.\n",
        "  \"\"\"\n",
        "  # Sample metric just computes a length match between each hypothesis and the\n",
        "  # reference. It ignores lp, domains, docs, and source.\n",
        "\n",
        "  del lp, domains, docs, src\n",
        "\n",
        "  ref_lens = np.array([len(r) for r in ref])\n",
        "  scores = {}\n",
        "  for sysname, hyp in hyps.items():\n",
        "    hyp_lens = np.array([len(h) for h in hyp])\n",
        "    deltas = np.abs(ref_lens - hyp_lens) / (ref_lens + 1)\n",
        "    scores[sysname] = -deltas if level == 'seg' else [-deltas.mean()]\n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm1F1I3YVCGI"
      },
      "outputs": [],
      "source": [
        "# @title Load EvalSets\n",
        "\n",
        "wmt23_lps = ['en-de', 'he-en', 'zh-en']\n",
        "evs_dict = {('wmt23', lp): data.EvalSet('wmt23', lp, True) for lp in wmt23_lps}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qRH8Y-iMFH5"
      },
      "outputs": [],
      "source": [
        "# @title Add metric scores to EvalSets\n",
        "\n",
        "# Compute scores for each language pair, and add to the appropriate EvalSet.\n",
        "# Setting replace=True makes this work if we want to iterate over different\n",
        "# versions of the metric.\n",
        "\n",
        "metric_name = 'lendiff'\n",
        "\n",
        "for lp in wmt23_lps:\n",
        "  evs = evs_dict[('wmt23', lp)]\n",
        "  for refname, ref in evs.all_refs.items():\n",
        "    sys_scores = NewMetric(\n",
        "        'sys', evs.lp, evs.domains, evs.docs, evs.src, ref, evs.sys_outputs)\n",
        "    seg_scores = NewMetric(\n",
        "        'seg', evs.lp, evs.domains, evs.docs, evs.src, ref, evs.sys_outputs)\n",
        "    evs.AddMetric(metric_name, {refname}, 'sys', sys_scores, replace=True)\n",
        "    evs.AddMetric(metric_name, {refname}, 'seg', seg_scores, replace=True)\n",
        "\n",
        "# Add new metric to the primary lists, so it will get picked up when tasks get\n",
        "# run with primary=True (avoiding having to evaluate all contrastive\n",
        "# submissions as well).\n",
        "\n",
        "for evs in evs_dict.values():\n",
        "  evs.SetPrimaryMetrics(evs.primary_metrics | {metric_name})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHtzjRQgXcs2"
      },
      "outputs": [],
      "source": [
        "# @title Generate results with new metric\n",
        "\n",
        "# For a first pass we turn off significance testing.\n",
        "\n",
        "wmt23_tasks, wts = tasks.WMT23(wmt23_lps, k=0)\n",
        "\n",
        "# Takes about 3 minutes.\n",
        "new_results = wmt23_tasks.Run(eval_set_dict=evs_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FDMKPU4d97V"
      },
      "outputs": [],
      "source": [
        "# @title Print results\n",
        "\n",
        "# Results show all primary metrics, along with the new 'lendiff' metric.\n",
        "\n",
        "avg_corrs = new_results.AverageCorrs(wts)\n",
        "\n",
        "table = new_results.Table(\n",
        "    metrics=list(avg_corrs),\n",
        "    initial_column=avg_corrs,\n",
        "    initial_column_header='avg-corr',\n",
        "    attr_list=['lang', 'level', 'corr_fcn'],\n",
        "    nicknames={'KendallWithTiesOpt': 'acc-t'},\n",
        "    fmt='text',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0elPs7kuXFO"
      },
      "outputs": [],
      "source": [
        "# @title Compare with significance\n",
        "\n",
        "# For speed reasons, limit comparison to the two metrics that bracket lendiff\n",
        "# in the average-correlation ranking.\n",
        "for evs in evs_dict.values():\n",
        "  evs.SetPrimaryMetrics({'Random-sysname', 'lendiff', 'eBLEU'})\n",
        "\n",
        "# Run the significance test. Set k=1000 for a more realistic comparison. This\n",
        "# takes about 2 minutes with k=50.\n",
        "wmt23_tasks, wts = tasks.WMT23(wmt23_lps, k=50)\n",
        "new_results = wmt23_tasks.Run(eval_set_dict=evs_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkFH9_xJwjOF"
      },
      "outputs": [],
      "source": [
        "# @title Print significance results\n",
        "\n",
        "avg_corrs, matrix = new_results.AverageCorrMatrix(main_task_weights)\n",
        "\n",
        "table = new_results.Table(\n",
        "    metrics=list(avg_corrs),\n",
        "    initial_column=avg_corrs,\n",
        "    initial_column_header='avg-corr',\n",
        "    attr_list=['lang', 'level', 'corr_fcn'],\n",
        "    nicknames={'KendallWithTiesOpt': 'acc-t'},\n",
        "    fmt='text',\n",
        "    baselines_metainfo=meta_info.WMT23)\n",
        "\n",
        "# The table indicates that lendiff and eBLEU are in the same significance\n",
        "# cluster ahead of Random-sysname.\n",
        "print(table)\n",
        "print()\n",
        "\n",
        "# Print the p-value matrix for the three pairwise comparisons used to assign\n",
        "# significance clusters.\n",
        "print(tasks.MatrixString(avg_corrs, matrix, probs=True))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1UgUZ35EdmwwuDljJMtlz5vAaOT4blX8J",
          "timestamp": 1699484321090
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
